{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtsxbISHTwbZabOlq4A5Dp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/textual-data-analysis-course/blob/main/tda_2025_exercise_task_2_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example solution for exercise task 2\n",
        "\n",
        "## Task 2 description\n",
        "\n",
        "Read the paper [The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale](https://arxiv.org/abs/2406.17557) (Penedo *et al.* 2024), which introduces the FineWeb and FineWeb-Edu datasets (version 1). Answer the following questions:\n",
        "\n",
        "* What are the key processing steps to create the FineWeb data from crawl sources, and what is the most important tool or method to implement each?\n",
        "* Does the processing to create the FineWeb data omit any of the processing steps discussed on the lecture, and does it add any? (what?)\n",
        "* What are the key differences between the FineWeb and FineWeb-edu datasets, and what are the key steps to create the latter from the former?\n"
      ],
      "metadata": {
        "id": "bTJ5HbKpbKNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2 example solution\n",
        "\n",
        "**Question**: What are the key processing steps to create the FineWeb data from crawl sources, and what is the most important tool or method to implement each?\n",
        "\n",
        "**Example answer**: The following steps and tools/methods are listed in Section 3 of the paper:\n",
        "\n",
        "* Text extraction (3.2) using trafilatura\n",
        "* Base filtering (3.3), including\n",
        "  * URL-based filtering using the UT1 blocklist\n",
        "  * fastText language classifier\n",
        "  * heuristic quality and repetition filters from MassiveText\n",
        "* Deduplication (3.4) using MinHash for fuzzy dedup in individual crawls\n",
        "* Additional heuristic filters from C4 (3.5)\n",
        "* Additional newly developed heuristic filters (3.6)\n",
        "* Personal Identifiable Information (PII) removal (3.7) \"anonymizing email and public IP addresses\"\n",
        "\n",
        "---\n",
        "\n",
        "**Question**: Does the processing to create the FineWeb data omit any of the processing steps discussed on the lecture, and does it add any? (what?)\n",
        "\n",
        "**Example answer**: on the lecture, the following processing steps to create clean unannotated text corpora from crawl data were listed:\n",
        "\n",
        "* Text extraction from HTML, excluding boilerplate\n",
        "* Exact duplicate removal\n",
        "* Near-duplicate removal\n",
        "* Heuristic quality filtering\n",
        "* Language model-based quality filtering\n",
        "* URL-based filtering\n",
        "* Content-based toxicity filtering\n",
        "* Content quality filtering\n",
        "* Personal information masking\n",
        "\n",
        "Compared to the steps listed here, the primary FineWeb processing adds **language classification** and omits **exact duplicate removal** (as a separate step from near-duplicate removal), **language model-based quality filtering**, and **content-based toxicity filtering**, also postponing **content quality filtering** to the step creating FineWeb-edu from the primary FineWeb data.\n",
        "\n",
        "---\n",
        "\n",
        "**Question**: What are the key differences between the FineWeb and FineWeb-edu datasets, and what are the key steps to create the latter from the former?\n",
        "\n",
        "**Example answer**: FineWeb-Edu is a subset of educational text filtered from FineWeb. The key steps to build it were (Section 4)\n",
        "\n",
        "* Using Llama-3-70B-Instruct to score 460,000 webpages FineWeb for their educational quality on a scale from 0 to 5\n",
        "* Fine-tunining a linear regression model on top of an embedding model on the synthetic annotations\n",
        "* Applying the classifier to all of FineWeb with a fixed threshold to identify an \"educational\" subset"
      ],
      "metadata": {
        "id": "AGHoGSTRi8W2"
      }
    }
  ]
}