{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYnXY9XVoa9ClAqsffIBiB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/textual-data-analysis-course/blob/main/llms_using_transformers_library.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using LLMs with the `transformers` library\n",
        "\n",
        "This notebook illustrates basic use of large language models using the Hugging Face `transformers` library."
      ],
      "metadata": {
        "id": "8-A6BxAdJI8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Setup\n",
        "\n",
        "You can change the model used throughout the notebook here:"
      ],
      "metadata": {
        "id": "VFwZWq6CXcpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'HuggingFaceTB/SmolLM2-135M-Instruct'\n",
        "#MODEL_NAME = 'HuggingFaceTB/SmolLM2-1.7B-Instruct'"
      ],
      "metadata": {
        "id": "BPrEPVUvNvbo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can change the prompt used in many of the examples here:"
      ],
      "metadata": {
        "id": "WDHiWBphfLFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = 'The best advice I ever got was:'"
      ],
      "metadata": {
        "id": "hLKQj75TfUq2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, the `transformers` library logs a variety of warning messages even when used correctly (e.g. \"Setting `pad_token_id` to `eos_token_id`). We'll here configure logging as follows to suppress these messages.\n",
        "\n",
        "**NOTE**: you should generally _not_ do this as some of the warnings may signal problems in your code."
      ],
      "metadata": {
        "id": "yOvcvVi_ZhA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "transformers.logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "e_vZ70WDV9Ke"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Minimal generation example\n",
        "\n",
        "The [`pipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines) class provides a high-level abstraction for a variety of tasks.\n",
        "\n",
        "To create a pipeline for text generation, simply invoke the `pipeline` constructor with the `text-generation` argument and the name of a model that supports text generation from the Hugging Face [models repository](https://huggingface.co/models?pipeline_tag=text-generation).\n",
        "\n",
        "To support loading larger models, we'll here also provide the arguments `device_map='auto'` and `torch_dtype='auto'`. If you're interested in knowing more about this, you can read the `accelerate` documentation on [Loading big models into memory](https://huggingface.co/docs/accelerate/v1.4.0/concept_guides/big_model_inference)."
      ],
      "metadata": {
        "id": "a--KBk5DJlNv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-T8QIBe1DwH",
        "outputId": "08f47630-2fa3-4d2c-ca61-886aea63e432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\n",
        "    'text-generation',\n",
        "    MODEL_NAME,\n",
        "    device_map='auto',\n",
        "    torch_dtype='auto',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Invoking a text generation pipeline with a prompt will return generated outputs, a list of dictionaries where the generated text is given as `'generated_text'`."
      ],
      "metadata": {
        "id": "3LOq0LGAbI2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = pipe(PROMPT)\n",
        "\n",
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9dUY2Pna1eY",
        "outputId": "b4c1d7db-99f6-47dd-8a24-7ce14058f9b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best advice I ever got was: \"Don't be afraid to ask for help.\"\n",
            "\n",
            "I'm not sure what to do with\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Example without `pipeline`\n",
        "\n",
        "We can run the same generation using the tokenizer and model explicity, as follows. First, we could directly load these using [\"auto\" classes](https://huggingface.co/docs/transformers/en/model_doc/auto):\n",
        "\n",
        "```\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "```"
      ],
      "metadata": {
        "id": "ji8eU2kaL5GQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, to avoid loading the model twice, we'll instead just grab the tokenizer and model from the pipeline we loaded earlier."
      ],
      "metadata": {
        "id": "bHAKZe-Yfj3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = pipe.tokenizer\n",
        "model = pipe.model"
      ],
      "metadata": {
        "id": "XNExwn66ML-e"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then get the same result as the pipeline produced using `tokenizer` and `model.generate` explicitly like this:"
      ],
      "metadata": {
        "id": "n0P18UhwgeJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'The best advice I ever got was:'\n",
        "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "outputs = model.generate(**inputs)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnyZh0ywa3aU",
        "outputId": "ae69a68e-03d1-457b-8984-f47fb1504bf4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best advice I ever got was: \"Don't be afraid to ask for help.\"\n",
            "\n",
            "I'm not sure what to do with\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `pipeline` class takes care of encoding the text into token indices for the model and decoding the output back into text, but you should keep in mind that the model doesn't deal with text but rather these indices:"
      ],
      "metadata": {
        "id": "TQ5W_VmCOq2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs.input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcU0x9oYPJUW",
        "outputId": "4506ec78-dab4-49d3-aec7-091f58c4ca19"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 504, 1450, 5042,  339, 2042, 3363,  436,   42]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9dFXrGXPM7P",
        "outputId": "1b226b6d-8991-433d-c249-963c2af17092"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  504,  1450,  5042,   339,  2042,  3363,   436,    42,   476,  8084,\n",
            "          982,   325, 11830,   288,  1998,   327,   724,  1270,   198,   198,\n",
            "           57,  5248,   441,  2090,   732,   288,   536,   351])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see the individual tokens that these indices (ids) correspond to by using `tokenizer.convert_ids_to_tokens`. (For many models you'll see encoded characters such as `Ġ` for space; [this is not an error](https://github.com/facebookresearch/fairseq/issues/1716))"
      ],
      "metadata": {
        "id": "zjfeXrSSZ3yN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.convert_ids_to_tokens(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKEKdBLXZ98f",
        "outputId": "61e72792-42d8-4421-fa8e-2da38b6baf18"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'Ġbest', 'Ġadvice', 'ĠI', 'Ġever', 'Ġgot', 'Ġwas', ':', 'Ġ\"', 'Don', \"'t\", 'Ġbe', 'Ġafraid', 'Ġto', 'Ġask', 'Ġfor', 'Ġhelp', '.\"', 'Ċ', 'Ċ', 'I', \"'m\", 'Ġnot', 'Ġsure', 'Ġwhat', 'Ġto', 'Ġdo', 'Ġwith']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Aside: where is my model?\n",
        "\n",
        "The `pipeline` class automates many aspects of model loading, including where it is loaded. Here's one way to check:"
      ],
      "metadata": {
        "id": "mDkljRzenDDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "print(model.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76VwZNG9ndLg",
        "outputId": "17781efc-3d22-4bd9-a403-0834fd37716d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're intending to run on GPU and see `cpu` above, you might want to try Runtime -> Change runtime type in the colab menu.\n",
        "\n",
        "If you've loaded a larger model with `device_map='auto'` or similar, you can get further details on the placement like this:"
      ],
      "metadata": {
        "id": "cQkDohqDoy_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.hf_device_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV_S52u9poOg",
        "outputId": "0ff55663-c960-40cc-f7d9-00060fb07d20"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'': 'cpu'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(If you see anything with the value `disk` above, part of the model has been offloaded onto disk, which may make things very slow.)"
      ],
      "metadata": {
        "id": "3C-oZmXmriJi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Generation parameters\n",
        "\n",
        "Both `model.generate` and text generation `pipeline` calls support a broad range of parameters that control generation.\n",
        "\n",
        "We'll look at some key ones here; for the full list refer to [the documentation](https://huggingface.co/docs/transformers/v4.48.2/en/main_classes/text_generation#transformers.GenerationConfig)."
      ],
      "metadata": {
        "id": "UDuI8yHDTEMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Output length\n",
        "\n",
        "The parameters `min_length`, `min_new_tokens`, `max_length`, and `max_new_tokens` control the minimum and maximum length of the generated output in tokens. The arguments with `_new_` ignore the length of the prompt.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "haYAUqxTd1wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = pipe(\n",
        "    prompt,\n",
        "    min_new_tokens=50,\n",
        "    max_new_tokens=100,\n",
        ")\n",
        "\n",
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVf1cR_yTDur",
        "outputId": "6a944a52-df7c-4333-9ffe-0f1f15729a17"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best advice I ever got was: \"Don't be afraid to ask for help.\"\n",
            "\n",
            "I'm not sure what to do with my life now. I'm not sure what to do with my life. I'm not sure what to do with my life. I'm not sure what to do with my life. I'm not sure what to do with my life. I'm not sure what to do with my life. I'm not sure what to do with my life. I'm not sure what to do with my life\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sampling\n",
        "\n",
        "By default, generation uses a _greedy_ decoding strategy that always picks the most likely next token, which may result in formulaic and repetitive generations.\n",
        "\n",
        "When called with `do_sample=True`, the next token is instead sampled from the probability distribution predicted by the model. (Note that with this means you'll get different outputs every time you run generation.)"
      ],
      "metadata": {
        "id": "mfkoXFoPfF5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = pipe(\n",
        "    prompt,\n",
        "    min_new_tokens=50,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdEG5-_1geuk",
        "outputId": "c24b9e05-6636-40cb-801e-b9e03922c8a1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best advice I ever got was: “Be prepared.”\n",
            "\n",
            "So, what are some of the things you already have ready or prepared that, when pressed, could change your life in big ways?\n",
            "\n",
            "When planning for a career transition, and especially in recent years, one or more of these things have become crucial. One way to assess your readiness now, and perhaps, to prepare your next career move, is to conduct a thorough job search and interview process. Take 3-5 months to gather relevant information, be familiar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a fairly large number of parameters that control the sampling strategy (see \"Parameters for manipulation of the model output logits\" [here](https://huggingface.co/docs/transformers/v4.48.2/en/main_classes/text_generation#transformers.GenerationConfig) for details).\n",
        "\n",
        "A key parameter is `temperature`:\n",
        "\n",
        "* values approaching zero approximate greedy sampling\n",
        "* values < 1 assign more probability mass to likely tokens\n",
        "* value 1 samples from the unmodified distribution\n",
        "* values > 1 assign more probability mass to unlikely tokens\n",
        "\n",
        "Intuitively, lower values give more predictable output and higher values more \"surprising\" or \"creative\" output. Values below the default of 1.0 (e.g. 0.5 or 0.7) are commonly used for `temperature`. Very high values are unlikely to produce useful output, but may provide insight into the generation process by understanding how it breaks down:"
      ],
      "metadata": {
        "id": "EGvsm388hdcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = pipe(\n",
        "    prompt,\n",
        "    min_new_tokens=50,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    temperature=100.0,\n",
        ")\n",
        "\n",
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzTnPC96hw4t",
        "outputId": "3ad28c33-8dee-4a45-a38b-63eda5d6291f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best advice I ever got was: to drink three sodas in their original bottles only under very restricted amounts just outside or after that one too large piece will cost fifty fifty so give up what can they. A dollar more each on other ones so it might seem there might. We will find nothing of such bad behaviour now at lunch too or for two the remainder may give you ten to die if given this wrong treatment but not twenty thirty per since twenty per will take eighteen, so for such situation for all twenty is reasonable too on both at\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Using chat models"
      ],
      "metadata": {
        "id": "mn4Jiin4keWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instruction- or chat-tuned models are commonly trained with special tokens to differentiate user input from model output, for example `<|im_start|>` and `<|im_end|>` in [ChatML](https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md).\n",
        "\n",
        "Special tokens and their usage conventions can differ between models, which means that switching models could potentially require writing model-specific code to format input. To make this easier, Hugging Face tokenizers introduced chat templates that implement that formatting.\n",
        "\n",
        "Basic usage is illustrated below. For more details, see the [Chat Templates documentation](https://huggingface.co/docs/transformers/main/en/chat_templating)."
      ],
      "metadata": {
        "id": "OwgQIpqPqL4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    { 'role': 'user', 'content': 'What is the capital of Finland?' }\n",
        "]\n",
        "\n",
        "print(tokenizer.apply_chat_template(messages, tokenize=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ze3-N3Rnk49x",
        "outputId": "ea7c179a-c968-4b0e-c792-446d625a7371"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|im_start|>system\n",
            "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
            "<|im_start|>user\n",
            "What is the capital of Finland?<|im_end|>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on the model, you may also see an added system message above. Here's how that would look for another model:"
      ],
      "metadata": {
        "id": "IKWDzzMwt_5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "other_model = 'HuggingFaceH4/zephyr-7b-beta'\n",
        "other_tokenizer = AutoTokenizer.from_pretrained(other_model)\n",
        "\n",
        "print(other_tokenizer.apply_chat_template(messages, tokenize=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsL2PGHBuD7H",
        "outputId": "020dfa38-fece-4daf-a832-ec54da45e72b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|user|>\n",
            "What is the capital of Finland?</s>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `apply_chat_template` function returns a string that can be tokenized and used for generation as usual. We here also add the `add_generation_prompt=True` argument to assure that the model continues the prompt with an assistant response rather than e.g. continuing the user message. (For more details on this, see [the documentation](https://huggingface.co/docs/transformers/main/en/chat_templating))."
      ],
      "metadata": {
        "id": "W-ikESeJv-k4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "inputs = tokenizer(input_text, return_tensors='pt').to(model.device)\n",
        "outputs = model.generate(**inputs)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyCcSLyNu7w2",
        "outputId": "f648f76a-ea8f-4e5c-baa4-d9a55ef72c50"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|im_start|>system\n",
            "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
            "<|im_start|>user\n",
            "What is the capital of Finland?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "The capital of Finland is Helsinki.<|im_end|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can of course also use a `pipeline` with text formatted by `apply_chat_template`:"
      ],
      "metadata": {
        "id": "pSwxlLuTaGCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(input_text)[0]['generated_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "eINx6PUGuvz4",
        "outputId": "de3b7375-b370-4d15-dc2b-941775b8f80c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|im_start|>system\\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\\n<|im_start|>user\\nWhat is the capital of Finland?<|im_end|>\\n<|im_start|>assistant\\nThe capital of Finland is Helsinki.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In recent versions of the `transformers` library, you can directly invoke a `pipeline` with an instruct- or chat- capable model and tokenizer. In this case, the templates will be used automatically to process input and output:"
      ],
      "metadata": {
        "id": "OsEKiH58yrAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = pipe(messages)\n",
        "outputs[0]['generated_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjOfMWDfz-Gy",
        "outputId": "b1fddd6f-866a-43b5-8f1a-2866d8c6efab"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'user', 'content': 'What is the capital of Finland?'},\n",
              " {'role': 'assistant', 'content': 'The capital of Finland is Helsinki.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that even instruction- or chat-tuned models can revert to standard \"continuation\" generation if you prompt them without the expected template."
      ],
      "metadata": {
        "id": "CbZALQORW_ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe('What is the capital of Finland?', min_new_tokens=10, max_new_tokens=20)[0]['generated_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "uecF10ItXOCV",
        "outputId": "cfaaf917-40b0-4bf3-95ce-b6092e9625e5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is the capital of Finland?\\n\\nA: Finland is a country in Northern Europe. It is located in the northern part of'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}