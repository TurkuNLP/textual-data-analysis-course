{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2+"
    },
    "colab": {
      "name": "02_boolean_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/textual-data-analysis-course/blob/main/ir_and_boolean_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-V1vx62ntdg"
      },
      "source": [
        "# Information Retrieval\n",
        "\n",
        "* Text search engines\n",
        "  * Term-document matrices / inverted index\n",
        "  * Dense vector representations, especially NN-based\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "o9TPr7snnRCH"
      },
      "source": [
        "**Disclaimer:** some of the material is borrowed with thanks from http://www.cis.lmu.de/~hs/teach/14s/ir/\n",
        "\n",
        "# Boolean model\n",
        "\n",
        "* The Boolean model is arguably the simplest model to\n",
        "  base an information retrieval system on\n",
        "* Still relevant b\n",
        "* Queries are Boolean expressions, e.g., *Caesar* and *Brutus*\n",
        "* The seach engine returns all documents that satisfy the Boolean expression, but there are exceptions:\n",
        "  * e.g. page contains variant of query words: morphology, spelling correction, synonyms\n",
        "\n",
        "## Term-document matrix\n",
        "\n",
        "* Rows: terms\n",
        "* Columns: document\n",
        "* $M_{ij}=1$ if term *i* present in document *j*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "1ICwFimQnRCO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d1096f-ad8e-4bb0-b943-ad752b8b272f"
      },
      "source": [
        "# We will not be using much of sklearn in the course, but this is an easy way to illustrate\n",
        "# the basic ideas\n",
        "# soon enough we will switch to the transformers code and stay there\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "documents=[\"This is a silly example\",\"A better example\",\"Nothing to see here\",\"This is a great and long example\"]\n",
        "cv=CountVectorizer(lowercase=False,binary=True)\n",
        "print(\"Term-document matrix:\\n\")\n",
        "td_matrix=cv.fit_transform(documents).todense().T   #.T transposes the matrix, sklearn maintains document-term\n",
        "print(td_matrix)\n",
        "print(\"\\nIDX -> terms mapping:\\n\")\n",
        "print(cv.get_feature_names())\n",
        "print(\"\\nterm -> IDX mapping:\\n\")\n",
        "print(cv.vocabulary_) # note the _ at the end\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term-document matrix:\n",
            "\n",
            "[[0 0 1 0]\n",
            " [1 0 0 1]\n",
            " [0 0 0 1]\n",
            " [0 1 0 0]\n",
            " [1 1 0 1]\n",
            " [0 0 0 1]\n",
            " [0 0 1 0]\n",
            " [1 0 0 1]\n",
            " [0 0 0 1]\n",
            " [0 0 1 0]\n",
            " [1 0 0 0]\n",
            " [0 0 1 0]]\n",
            "\n",
            "IDX -> terms mapping:\n",
            "\n",
            "['Nothing', 'This', 'and', 'better', 'example', 'great', 'here', 'is', 'long', 'see', 'silly', 'to']\n",
            "\n",
            "term -> IDX mapping:\n",
            "\n",
            "{'This': 1, 'is': 7, 'silly': 10, 'example': 4, 'better': 3, 'Nothing': 0, 'to': 11, 'see': 9, 'here': 6, 'great': 5, 'and': 2, 'long': 8}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "x4KMWUGinRCR"
      },
      "source": [
        "* Every row is an *incidence vector* of a term - which documents the term appears in\n",
        "* Inverted index -> direct path from a term to documents\n",
        "* Boolean retrieval in its simplest form:\n",
        "  * Boolean operations on incidence vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "mc4nsEFGnRCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65e12b21-5040-4de5-8b83-4adc88cfd46c"
      },
      "source": [
        "t2i=cv.vocabulary_\n",
        "print(\"example\")\n",
        "print(td_matrix[t2i[\"example\"]])\n",
        "print()\n",
        "print(\"example and great\")\n",
        "print(td_matrix[t2i[\"example\"]] & td_matrix[t2i[\"great\"]])\n",
        "print()\n",
        "print(\"not example\")\n",
        "print(1-td_matrix[t2i[\"example\"]]) #1-x does the negation in our case\n",
        "print()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example\n",
            "[[1 1 0 1]]\n",
            "\n",
            "example and great\n",
            "[[0 0 0 1]]\n",
            "\n",
            "not example\n",
            "[[0 0 1 0]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "MB3RwEa1nRCU"
      },
      "source": [
        "## DIY search engine\n",
        "\n",
        "We can piece it together into an elementary search engine, you can check it out, but we won't go through this in the course\n",
        "\n",
        "* Accept queries like \"( not example or great ) and Nothing\"\n",
        "* Rewrite them into a Python expression\n",
        "* eval() that expression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "eX2wF-RknRCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a9d2f6e-90dc-4ec4-9f56-12e9008d0b9f"
      },
      "source": [
        "# Operators and, or, not become &, |, 1 -\n",
        "# Parentheses are left untouched\n",
        "# Everything else interpreted as a term and fed through td_matrix[t2i[\"...\"]]\n",
        "\n",
        "d={\"and\":\"&\",\"or\":\"|\",\"not\":\"1 -\",\"(\":\"(\",\")\":\")\"} # operator replacements\n",
        "def rew_token(t):\n",
        "    return d.get(t,f'td_matrix[t2i[\"{t}\"]]') #if it is operator, replace, if not, then replace with td_matrix[t2i[\"{t}\"]]\n",
        "\n",
        "def rew_query(query): #rewrite every token in the query\n",
        "    return \" \".join(rew_token(t) for t in query.split())\n",
        "\n",
        "def test_query(query):\n",
        "    print(\"Query:\\\"\"+query+\"\\\"\")\n",
        "    print(\"Rewritten:\",rew_query(query))\n",
        "    print(\"Matching:\",eval(rew_query(query)))\n",
        "    print()\n",
        "\n",
        "test_query(\"example and not Nothing\")\n",
        "test_query(\"not example or great\")\n",
        "test_query(\"( not example or great ) and Nothing\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\"example and not Nothing\"\n",
            "Rewritten: td_matrix[t2i[\"example\"]] & 1 - td_matrix[t2i[\"Nothing\"]]\n",
            "Matching: [[1 1 0 1]]\n",
            "\n",
            "Query:\"not example or great\"\n",
            "Rewritten: 1 - td_matrix[t2i[\"example\"]] | td_matrix[t2i[\"great\"]]\n",
            "Matching: [[0 0 1 1]]\n",
            "\n",
            "Query:\"( not example or great ) and Nothing\"\n",
            "Rewritten: ( 1 - td_matrix[t2i[\"example\"]] | td_matrix[t2i[\"great\"]] ) & td_matrix[t2i[\"Nothing\"]]\n",
            "Matching: [[0 0 1 0]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "wJkanF3dnRCY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "247d78d2-4236-4b2d-d925-4f51012e92ec"
      },
      "source": [
        "# Should match all documents, but crashes instead\n",
        "# This one you will fix as an exercise\n",
        "import traceback\n",
        "try:\n",
        "    test_query(\"not awordwhichdoesnotexist\") #should match all documents\n",
        "except:\n",
        "    traceback.print_exc() #I only need to do this because of IPython"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query:\"not awordwhichdoesnotexist\"\n",
            "Rewritten: 1 - td_matrix[t2i[\"awordwhichdoesnotexist\"]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-25-a02963e0a344>\", line 5, in <module>\n",
            "    test_query(\"not awordwhichdoesnotexist\") #should match all documents\n",
            "  File \"<ipython-input-24-ade8113ddedf>\", line 15, in test_query\n",
            "    print(\"Matching:\",eval(rew_query(query)))\n",
            "  File \"<string>\", line 1, in <module>\n",
            "KeyError: 'awordwhichdoesnotexist'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "MNzTVUaynRCZ"
      },
      "source": [
        "* We now have a rudimentary boolean IR system\n",
        "* It is not that great but ready in < 10 lines of code in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Z81tk-KrnRCZ"
      },
      "source": [
        "# Limitations of this elementary implementation (of this elementary model)\n",
        "\n",
        "## Size constraints\n",
        "* Term-document matrix is vocabulary size times document set size\n",
        "* 1M documents, each 1000 words\n",
        "* 1,000,000 x 1,000 = 1,000,000,000 words of running text\n",
        "* 6 bytes per word -> ~6GB in size\n",
        "* Assume 500,000 unique terms\n",
        "* Term-document matrix has 1,000,000 x 500,000 / 8 / 1024 / 1024 / 1024 -> ~60GB\n",
        "* 60GB of space to index a collection of 6GB of text!\n",
        "* ...but most of this are zeros...\n",
        "* Sparse representation: only remember the non-zero entries\n",
        "* For every term, remember a (usually sorted) list of documents in which it appears\n",
        "* This is the famous **inverted index**\n",
        "* Scipy sparse formats: https://docs.scipy.org/doc/scipy/reference/sparse.html\n",
        "  * *CSC* for every column remember the list of rows\n",
        "  * *CSR* for every row remember the list of columns\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "aDsdcUiznRCd"
      },
      "source": [
        "## Phrase and proximity queries\n",
        "\n",
        "* \"*Stanford university*\" - not the same thing as *Stanford AND university*\n",
        "* The basic inverted index of no help\n",
        "* Index all word bigrams\n",
        "  * \"*Stanford university*\" becomes a term\n",
        "  * but then you have problem with *\"Stanford university courses\"*\n",
        "  * \"to be or not to be\" -> \"to be\" and \"be or\" and \"or not\" and \"not to\" + postprocessing\n",
        "  * Not a great solution:\n",
        "  * Massive waste of space: blows up the index size quadratically\n",
        "  * Needs postprocessing to weed out the false hits\n",
        "* Positional index\n",
        "  * For every term, and every document, remember a list of the positions, not just \"1\"\n",
        "  * A modification of the sorted list intersection algorithm to answer the query\n",
        "  * Also allows proximity queries \"X within N words from Y\"\n",
        "  * Quite heavy computationally\n",
        "* Combined index\n",
        "  * Most common biwords indexed directly\n",
        "  * Rest solved with positional indexing\n",
        "  * Compromise between the two"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "l6bzYdw4nRCe"
      },
      "source": [
        "# Result ranking\n",
        "\n",
        "* Obviously useful for large document collections\n",
        "* Come up with a number describing the fit of a document to a query\n",
        "* Return top-N documents\n",
        "* Basic observations:\n",
        "  * The more query terms hit, the more relevant the document is\n",
        "  * The more times the query terms hit in the document, the more relevant the document is\n",
        "  * Rare terms are more informative than common terms\n",
        "* Do not store 0/1, store count / TF-IDF weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Rr0qBQHRnRCl"
      },
      "source": [
        "### Informativeness of terms\n",
        "\n",
        "* Not all words are equally informative\n",
        "* Rare words are clearly much more informative than common ones\n",
        "* The query *\"jabberwocky movie cast\"* should put more weight on *jabberwocky*\n",
        "* What we want:\n",
        "  * High positive weight for rare terms\n",
        "  * Low positive weight for common terms\n",
        "* Typical way: IDF *inverse document frequency*\n",
        "  * df_t is the *document frequency* of *t* - number of documents *t* appear in\n",
        "  * $IDF_t=\\frac{N}{df_t}$ inverse document frequency of *t*\n",
        "  * Usually one would squeeze this through log so\n",
        "  * $IDF_t=log_{10}(\\frac{N}{df_t})$\n",
        "\n",
        "## Tf.Idf\n",
        "\n",
        "* An extremely common weighting scheme in IR\n",
        "* Product of term's tf (log-squeezed) with the term's idf (log-squeezed)\n",
        "* Gives a score of that term's hit in a given document\n",
        "  * $(1+log_{10}tf_t)\\cdot log_{10}\\frac{N}{df_t}$\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "zbLtUWlLnRCp"
      },
      "source": [
        "# Vector-space model\n",
        "\n",
        "* So far we looked at term vectors (rows of the term-document matrix)\n",
        "* We could also think about document vectors (columns of the term-document matrix)\n",
        "* Documents are vectors in a high-dimensional space, terms are the dimensions\n",
        "* Document vectors very high-dimensional but very very sparse\n",
        "* Same for queries - queries can also be seen as vectors in a high-dimensional space\n",
        "* Search:\n",
        "  * similarity between query vector and document vector\n",
        "  * higher similarity means better hit (rank higher)\n",
        "  \n",
        "## Similarity measures\n",
        "\n",
        "* Similarity -> negative distance\n",
        "* Eucledian distance -> affected by vector length\n",
        "* Cosine similarity -> cosine of the angle between query and document vectors\n",
        "  * 1 for total similarity (angle 0)\n",
        "  * -1 for complete opposite\n",
        "  * we only have positive numbers in our vectors, so we are on the [0,1] scale not [-1,1]\n",
        "  * not sensitive to length\n",
        "  * incredibly easy to compute!\n",
        "  * $cos(u,v)=\\frac{u\\cdot v}{||u||\\cdot ||v||}=\\frac{\\sum_i v_i\\cdot u_i}{\\sqrt{\\sum_i u_i^2}\\sqrt{\\sum_i v_i^2}}$\n",
        "  * dot-product of normalized vectors - just multiply the numbers together, really\n",
        "  \n",
        "## Weighting schemes - document, query\n",
        "\n",
        "* Now we can apply different weighting to documents and queries\n",
        "* Typical choice:\n",
        "  * document: log tf, no idf, length-normalized - why no idf?\n",
        "  * query: log tf, log idf, length-normalized\n",
        "  * cosine similarity\n",
        "\n"
      ]
    }
  ]
}