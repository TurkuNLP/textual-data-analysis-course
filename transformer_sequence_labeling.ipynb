{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMp/Ggd/3UFTHjN2/+fdv6F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/textual-data-analysis-course/blob/main/transformer_sequence_labeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named entity recognition\n",
        "\n",
        "Let's train a transformer model on a Named Entity Recognition (NER) dataset."
      ],
      "metadata": {
        "id": "6z3NuDfrprZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Setup"
      ],
      "metadata": {
        "id": "adKOlw8frrlz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the required Python packages:"
      ],
      "metadata": {
        "id": "Ya7aNGYeqcql"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7t5cMszypelm"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet transformers datasets evaluate seqeval"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the libraries we'll be using here."
      ],
      "metadata": {
        "id": "UhlAurZOvpWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import transformers\n",
        "import evaluate\n",
        "\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "h19T7mGuvt8-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make things a bit more quiet. (This only affects what shows on screen when running. If you're debugging, you probably want to comment these out.)"
      ],
      "metadata": {
        "id": "LySPAWdSqjA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformers.utils.logging.set_verbosity_error()\n",
        "datasets.logging.set_verbosity_error()\n",
        "datasets.disable_progress_bar()"
      ],
      "metadata": {
        "id": "uo-wMrnKqpIW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Load dataset\n",
        "\n",
        "Load a dataset for training using `datasets`."
      ],
      "metadata": {
        "id": "kUKGKvnHqKt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = 'conll2003'\n",
        "\n",
        "builder = datasets.load_dataset_builder(DATASET)\n",
        "dataset = datasets.load_dataset(DATASET)"
      ],
      "metadata": {
        "id": "3lZDBHc7ppI6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at the description and dataset."
      ],
      "metadata": {
        "id": "fzvOrvMUrQW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(builder.info.description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PUGnArpq0I4",
        "outputId": "97bbf9f1-c031-4fe2-b75f-dd0693453d2e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on\n",
            "four types of named entities: persons, locations, organizations and names of miscellaneous entities that do\n",
            "not belong to the previous three groups.\n",
            "\n",
            "The CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on\n",
            "a separate line and there is an empty line after each sentence. The first item on each line is a word, the second\n",
            "a part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags\n",
            "and the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only\n",
            "if two phrases of the same type immediately follow each other, the first word of the second phrase will have tag\n",
            "B-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2\n",
            "tagging scheme, whereas the original dataset uses IOB1.\n",
            "\n",
            "For more details see https://www.clips.uantwerpen.be/conll2003/ner/ and https://www.aclweb.org/anthology/W03-0419\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY-ER5vErVEs",
        "outputId": "f8be54ae-474e-4dbe-d2e3-d73836e19796"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
            "        num_rows: 14041\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
            "        num_rows: 3250\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
            "        num_rows: 3453\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the conventional split into `train`, `validation`, and `test`.\n",
        "\n",
        "We're here only interested in the `tokens` and `ner_tags`. (In particular, the `ner_tags` and `chunk_tags` are included to support methods based on manually engineered features, and as such not highly relevant to the deep learning approach we're pursuing here.)\n",
        "\n",
        "Let's have a look at one example."
      ],
      "metadata": {
        "id": "wK0I81MzWfW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['train'][0]['tokens'])\n",
        "print(dataset['train'][0]['ner_tags'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg7YVgnCXglr",
        "outputId": "30311e3e-9bf5-4446-b436-132b2031391a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
            "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take note of the number of different labels and create mappings from label IDs to label strings and vice versa; we'll need these later."
      ],
      "metadata": {
        "id": "6DlQnP8N7EyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_names = dataset['train'].features['ner_tags'].feature.names\n",
        "print('Labels:', label_names)\n",
        "\n",
        "num_labels = len(label_names)\n",
        "id2label = { k: v for k, v in enumerate(label_names) }\n",
        "label2id = { v: k for k, v in enumerate(label_names) }\n",
        "\n",
        "print('Number of labels:', num_labels)\n",
        "print('id2label mapping:', id2label)\n",
        "print('labelid2 mapping:', label2id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHGjHsNf7I4E",
        "outputId": "60d9c110-b6f2-4cd0-f37c-41dbd24e187e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
            "Number of labels: 9\n",
            "id2label mapping: {0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC'}\n",
            "labelid2 mapping: {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see that example again, applying the label mapping:"
      ],
      "metadata": {
        "id": "fvxCFhkAYMXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token, tag_id in zip(dataset['train'][0]['tokens'], dataset['train'][0]['ner_tags']):\n",
        "    print(f'{token}\\t{id2label[tag_id]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPqVWzNyYSFZ",
        "outputId": "409cb4f3-b255-479c-96f4-14463b7816de"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EU\tB-ORG\n",
            "rejects\tO\n",
            "German\tB-MISC\n",
            "call\tO\n",
            "to\tO\n",
            "boycott\tO\n",
            "British\tB-MISC\n",
            "lamb\tO\n",
            ".\tO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Tokenize and vectorize dataset\n",
        "\n",
        "As in the [text classification notebook](https://github.com/TurkuNLP/textual-data-analysis-course/blob/main/text_classification_basic_example.ipynb), we'll first load the tokenizer that corresponds to the model that we want to use. `AutoTokenizer` is a convenience class that will return the appropriate tokenizer for the model it's given as an argument:"
      ],
      "metadata": {
        "id": "d2yaMXe7unE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'bert-base-cased'\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL)"
      ],
      "metadata": {
        "id": "IG2DtYoRvGWa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer will, most importantly, produce `input_ids`, which identify the tokens of the text.\n",
        "\n",
        "The BERT tokenizer also produces an `attention_mask`, which can be used to make the model ignore some tokens, and `token_type_ids`, which can differentiate parts of the input e.g. when it consists of two separate texts."
      ],
      "metadata": {
        "id": "yI3Ve7f3vfap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(tokenizer('this is an example sentence'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7sIEg4ivgoy",
        "outputId": "6d0a478d-aa8d-430a-d1b0-f08a20ef8e4a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attention_mask': [1, 1, 1, 1, 1, 1, 1],\n",
            " 'input_ids': [101, 1142, 1110, 1126, 1859, 5650, 102],\n",
            " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A key point here is to note that the data already has its own definition of \"token\", and the tokenizer may split some of those into parts:"
      ],
      "metadata": {
        "id": "Cz-TQwRsbR7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer('Turku is not in the vocabulary').input_ids\n",
        "\n",
        "\n",
        "print(input_ids)\n",
        "print()\n",
        "print(tokenizer.convert_ids_to_tokens(input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlJOXt_1a3oU",
        "outputId": "7f63870a-86bc-4a65-8f12-5eb932d06e9e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 17037, 4661, 1358, 1110, 1136, 1107, 1103, 18074, 102]\n",
            "\n",
            "['[CLS]', 'Tu', '##rk', '##u', 'is', 'not', 'in', 'the', 'vocabulary', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each sentence, our data consists of a tokenized list of strings (\"words\") rather than a single string. If we call the tokenizer with its default options, it interprets each token as a different example:"
      ],
      "metadata": {
        "id": "8pflRkNAeJD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['train'][0]['tokens'])\n",
        "print()\n",
        "\n",
        "for ids in tokenizer(dataset['train'][0]['tokens']).input_ids:\n",
        "    print(tokenizer.convert_ids_to_tokens(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLHE2uUWeNob",
        "outputId": "cc1147f0-04cc-4e64-fc0b-2ca4abfd4eba"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
            "\n",
            "['[CLS]', 'EU', '[SEP]']\n",
            "['[CLS]', 'rejects', '[SEP]']\n",
            "['[CLS]', 'German', '[SEP]']\n",
            "['[CLS]', 'call', '[SEP]']\n",
            "['[CLS]', 'to', '[SEP]']\n",
            "['[CLS]', 'boycott', '[SEP]']\n",
            "['[CLS]', 'British', '[SEP]']\n",
            "['[CLS]', 'la', '##mb', '[SEP]']\n",
            "['[CLS]', '.', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the correct mapping, we provide `is_split_into_words=True` to the tokenizer."
      ],
      "metadata": {
        "id": "Qg0gitvecGDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = 'Turku is not in the vocabulary'.split()\n",
        "tokenized = tokenizer(tokens, is_split_into_words=True)\n",
        "\n",
        "print(tokens)\n",
        "print()\n",
        "pprint(tokenizer.convert_ids_to_tokens(tokenized.input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVY4p2I3cg6q",
        "outputId": "8c401175-9b08-493e-a17b-d57c677227aa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Turku', 'is', 'not', 'in', 'the', 'vocabulary']\n",
            "\n",
            "['[CLS]', 'Tu', '##rk', '##u', 'is', 'not', 'in', 'the', 'vocabulary', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer also provides us with a mapping from the tokenizer tokens to \"original\" tokens (\"words\")"
      ],
      "metadata": {
        "id": "p_YMxebffxow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized.word_ids()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbdQNIcIf6qI",
        "outputId": "2e20adbc-479b-4753-c020-a16f6405e47b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, 0, 0, 0, 1, 2, 3, 4, 5, None]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With `is_split_into_words=True`, we can tokenize the input so that its tokens are compatible with the model, but the labels will be misaligned."
      ],
      "metadata": {
        "id": "yyKqaqW2xoai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import zip_longest\n",
        "\n",
        "token_ids = tokenizer(dataset['train'][0]['tokens'], is_split_into_words=True).input_ids\n",
        "tag_ids = dataset['train'][0]['ner_tags']\n",
        "\n",
        "for token_id, tag_id in zip_longest(token_ids, tag_ids):\n",
        "    token = tokenizer.convert_ids_to_tokens(token_id)\n",
        "    tag = id2label[tag_id] if tag_id is not None else None\n",
        "    print(f'{token}\\t{tag}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKC8KstnyEkw",
        "outputId": "dc81d355-505a-4ad2-ef67-f64c6b920d46"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]\tB-ORG\n",
            "EU\tO\n",
            "rejects\tB-MISC\n",
            "German\tO\n",
            "call\tO\n",
            "to\tO\n",
            "boycott\tB-MISC\n",
            "British\tO\n",
            "la\tO\n",
            "##mb\tNone\n",
            ".\tNone\n",
            "[SEP]\tNone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To resolve this, we'll borrow a function from [a transformers tutorial](https://huggingface.co/course/chapter7/2). Here, `-100` is a \"magic value\" for a label that pytorch ignores."
      ],
      "metadata": {
        "id": "HsmDsthgzGoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def align_labels_with_tokens(labels, word_ids):\n",
        "    new_labels = []\n",
        "    current_word = None\n",
        "    for word_id in word_ids:\n",
        "        if word_id != current_word:    # Start of a new word\n",
        "            current_word = word_id\n",
        "            label = -100 if word_id is None else labels[word_id]\n",
        "            new_labels.append(label)\n",
        "        elif word_id is None:          # Special token            \n",
        "            new_labels.append(-100)\n",
        "        else:                          # Same word as previous token\n",
        "            label = labels[word_id]\n",
        "            if label % 2 == 1:         # If label is B-XXX we change it to I-XXX\n",
        "                label += 1\n",
        "            new_labels.append(label)\n",
        "    return new_labels"
      ],
      "metadata": {
        "id": "S_cxn0BIzS4U"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also borrow a function for jointly tokenizing the text and aliging labels:"
      ],
      "metadata": {
        "id": "R8nx5IKK0A5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(inputs):\n",
        "    outputs = tokenizer(inputs['tokens'], truncation=True, is_split_into_words=True)\n",
        "    new_labels = []\n",
        "    for i, labels in enumerate(inputs['ner_tags']):\n",
        "        word_ids = outputs.word_ids(i)\n",
        "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
        "    outputs['labels'] = new_labels\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "rzQ1o5103usD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll then apply this to the whole dataset:"
      ],
      "metadata": {
        "id": "TljzveA14amt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "id": "6MQVEP6y4Z-y"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now these should match up:"
      ],
      "metadata": {
        "id": "YU_kJtqu7It2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = dataset['train'][0]['input_ids']\n",
        "tag_ids = dataset['train'][0]['labels']\n",
        "\n",
        "for token_id, tag_id in zip_longest(token_ids, tag_ids):\n",
        "    token = tokenizer.convert_ids_to_tokens(token_id)\n",
        "    tag = id2label[tag_id] if tag_id != -100 else None\n",
        "    print(f'{token}\\t{tag}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgRVPp6p7NjK",
        "outputId": "6977bb88-4fda-4d92-ae8f-124c64b886b6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]\tNone\n",
            "EU\tB-ORG\n",
            "rejects\tO\n",
            "German\tB-MISC\n",
            "call\tO\n",
            "to\tO\n",
            "boycott\tO\n",
            "British\tB-MISC\n",
            "la\tO\n",
            "##mb\tO\n",
            ".\tO\n",
            "[SEP]\tNone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Instantiate model\n",
        "\n",
        "Now, we'll instantiate a pretrained model with a sequence labeling head. In the `transformers` library, this class of models are named `...ModelForTokenClassification`. (cf. `...ModelForSequenceClassification`) We'll again use the `Auto` variant to get the appropriate class based on model name.\n",
        "\n",
        "**NOTE**: we need to provide the number of labels to `from_pretrained` so that the function knows the size of the output layer that is required. The `id2label` and `label2id` mappings allow the model to report its classification results in interpretable text labels."
      ],
      "metadata": {
        "id": "coct30Pmr0Ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = transformers.AutoModelForTokenClassification.from_pretrained(\n",
        "    MODEL,\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")"
      ],
      "metadata": {
        "id": "m5D1eK0FraRU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Training configuration\n",
        "\n",
        "To assess the progress and results of training, we'll use the standard `seqeval` library. We'll also need to introduce a function that takes model outputs and the labels from the dataset and calls the metric.\n",
        "\n",
        "Here, we'll again borrow from [the transformers tutorial](https://huggingface.co/course/chapter7/2):"
      ],
      "metadata": {
        "id": "GglUCNFm1f9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = evaluate.load('seqeval')\n",
        "\n",
        "\n",
        "def compute_metrics(outputs_and_labels):\n",
        "    outputs, labels = outputs_and_labels\n",
        "    predictions = outputs.argmax(axis=-1)\n",
        "\n",
        "    # Remove ignored index (special tokens) and convert to labels\n",
        "    true_labels = [[id2label[i] for i in label if i != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    all_metrics = metrics.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        'precision': all_metrics['overall_precision'],\n",
        "        'recall': all_metrics['overall_recall'],\n",
        "        'f1': all_metrics['overall_f1'],\n",
        "        'accuracy': all_metrics['overall_accuracy'],\n",
        "    }"
      ],
      "metadata": {
        "id": "Q1ph62MmsTxB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also need a collator for padding the examples to the same length to process them in batches."
      ],
      "metadata": {
        "id": "1cDpsGI_-2hO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "Hp9Cwl8X-6KA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `TrainingArguments` class configures many of the details of the model training. You may want to try optimizing the following hyperparameters to improve model performance:\n",
        "\n",
        "* `learning_rate`: the step size for weight updates\n",
        "* `per_device_train_batch_size`: number of examples per training batch\n",
        "* `max_steps`: the maximum number of steps to train for"
      ],
      "metadata": {
        "id": "5CMOftQi4nIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_args = transformers.TrainingArguments(\n",
        "    output_dir='checkpoints',\n",
        "    evaluation_strategy='steps',\n",
        "    logging_strategy='steps',\n",
        "    load_best_model_at_end=True,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        "    learning_rate=0.00002,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    max_steps=1500,\n",
        ")"
      ],
      "metadata": {
        "id": "T13HyLrh2oRE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we'll create a custom [callback](https://huggingface.co/docs/transformers/main_classes/callback) to store values logged during training so that we can more easily examine them later. (This is only needed for visualization and is not necessary to understand in detail.)"
      ],
      "metadata": {
        "id": "q-er8P2eAHHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class LogSavingCallback(transformers.TrainerCallback):\n",
        "    def on_train_begin(self, *args, **kwargs):\n",
        "        self.logs = defaultdict(list)\n",
        "        self.training = True\n",
        "\n",
        "    def on_train_end(self, *args, **kwargs):\n",
        "        self.training = False\n",
        "\n",
        "    def on_log(self, args, state, control, logs, model=None, **kwargs):\n",
        "        if self.training:\n",
        "            for k, v in logs.items():\n",
        "                if k != \"epoch\" or v not in self.logs[k]:\n",
        "                    self.logs[k].append(v)\n",
        "\n",
        "training_logs = LogSavingCallback()"
      ],
      "metadata": {
        "id": "DwMxldxOAUHD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Train (fine-tune) model"
      ],
      "metadata": {
        "id": "enKZualc5hzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    args=trainer_args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['validation'],\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[training_logs], \n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rauYFl45mi9",
        "outputId": "c9f4eaff-03e9-49da-a62f-543653a8642f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTCrH63X50T_",
        "outputId": "593cc68b-1a18-4395-b993-117e3bc70697"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, chunk_tags, ner_tags, pos_tags, tokens. If id, chunk_tags, ner_tags, pos_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 14041\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1500\n",
            "  Number of trainable parameters = 107726601\n",
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, chunk_tags, ner_tags, pos_tags, tokens. If id, chunk_tags, ner_tags, pos_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3250\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5826, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.11}\n",
            "{'eval_loss': 0.21804942190647125, 'eval_precision': 0.6797900262467191, 'eval_recall': 0.7409962975429149, 'eval_f1': 0.7090748047346808, 'eval_accuracy': 0.9404838994525225, 'eval_runtime': 12.0932, 'eval_samples_per_second': 268.747, 'eval_steps_per_second': 8.435, 'epoch': 0.11}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, chunk_tags, ner_tags, pos_tags, tokens. If id, chunk_tags, ner_tags, pos_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3250\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.1767, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.23}\n",
            "{'eval_loss': 0.12931352853775024, 'eval_precision': 0.7846743295019157, 'eval_recall': 0.861662739818243, 'eval_f1': 0.8213684126092885, 'eval_accuracy': 0.9632660269617943, 'eval_runtime': 12.4985, 'eval_samples_per_second': 260.03, 'eval_steps_per_second': 8.161, 'epoch': 0.23}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, chunk_tags, ner_tags, pos_tags, tokens. If id, chunk_tags, ner_tags, pos_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3250\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.1259, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.34}\n",
            "{'eval_loss': 0.10791640728712082, 'eval_precision': 0.8412216181643748, 'eval_recall': 0.8853921238640189, 'eval_f1': 0.8627418825844538, 'eval_accuracy': 0.9699623241302172, 'eval_runtime': 12.7872, 'eval_samples_per_second': 254.16, 'eval_steps_per_second': 7.977, 'epoch': 0.34}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, chunk_tags, ner_tags, pos_tags, tokens. If id, chunk_tags, ner_tags, pos_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3250\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.1017, 'learning_rate': 1.4666666666666666e-05, 'epoch': 0.46}\n",
            "{'eval_loss': 0.09646957367658615, 'eval_precision': 0.8561381074168798, 'eval_recall': 0.9013800067317401, 'eval_f1': 0.8781767502869322, 'eval_accuracy': 0.9732442455995761, 'eval_runtime': 13.8061, 'eval_samples_per_second': 235.404, 'eval_steps_per_second': 7.388, 'epoch': 0.46}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, chunk_tags, ner_tags, pos_tags, tokens. If id, chunk_tags, ner_tags, pos_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3250\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.0952, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.57}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to checkpoints/checkpoint-500\n",
            "Configuration saved in checkpoints/checkpoint-500/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.0812300369143486, 'eval_precision': 0.8759477335054041, 'eval_recall': 0.9138337260181757, 'eval_f1': 0.8944897454904869, 'eval_accuracy': 0.9757755931006064, 'eval_runtime': 10.9702, 'eval_samples_per_second': 296.256, 'eval_steps_per_second': 9.298, 'epoch': 0.57}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in checkpoints/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in checkpoints/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in checkpoints/checkpoint-500/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, chunk_tags, ner_tags, pos_tags, tokens. If id, chunk_tags, ner_tags, pos_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3250\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.0884, 'learning_rate': 1.2e-05, 'epoch': 0.68}\n",
            "{'eval_loss': 0.06943660974502563, 'eval_precision': 0.8799550633927139, 'eval_recall': 0.9227532817233255, 'eval_f1': 0.9008461348886881, 'eval_accuracy': 0.9790722317065992, 'eval_runtime': 11.0399, 'eval_samples_per_second': 294.388, 'eval_steps_per_second': 9.239, 'epoch': 0.68}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, chunk_tags, ner_tags, pos_tags, tokens. If id, chunk_tags, ner_tags, pos_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3250\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.087, 'learning_rate': 1.0666666666666667e-05, 'epoch': 0.8}\n",
            "{'eval_loss': 0.07665019482374191, 'eval_precision': 0.877176169940904, 'eval_recall': 0.9242679232581622, 'eval_f1': 0.9001065311808572, 'eval_accuracy': 0.9765997527521045, 'eval_runtime': 11.274, 'eval_samples_per_second': 288.275, 'eval_steps_per_second': 9.047, 'epoch': 0.8}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, chunk_tags, ner_tags, pos_tags, tokens. If id, chunk_tags, ner_tags, pos_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3250\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.0854, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.91}\n",
            "{'eval_loss': 0.06570718437433243, 'eval_precision': 0.9032416502946955, 'eval_recall': 0.928475260854931, 'eval_f1': 0.9156846473029046, 'eval_accuracy': 0.9815005592511921, 'eval_runtime': 11.1389, 'eval_samples_per_second': 291.77, 'eval_steps_per_second': 9.157, 'epoch': 0.91}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, chunk_tags, ner_tags, pos_tags, tokens. If id, chunk_tags, ner_tags, pos_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3250\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.0689, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.03}\n",
            "{'eval_loss': 0.06760372966527939, 'eval_precision': 0.8987610042386697, 'eval_recall': 0.927802086839448, 'eval_f1': 0.9130506790327922, 'eval_accuracy': 0.9804114911402837, 'eval_runtime': 11.1994, 'eval_samples_per_second': 290.193, 'eval_steps_per_second': 9.108, 'epoch': 1.03}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, chunk_tags, ner_tags, pos_tags, tokens. If id, chunk_tags, ner_tags, pos_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3250\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.0451, 'learning_rate': 6.666666666666667e-06, 'epoch': 1.14}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to checkpoints/checkpoint-1000\n",
            "Configuration saved in checkpoints/checkpoint-1000/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.0695732906460762, 'eval_precision': 0.9064394560052433, 'eval_recall': 0.9309996634129922, 'eval_f1': 0.9185554171855541, 'eval_accuracy': 0.9810296108789074, 'eval_runtime': 11.4541, 'eval_samples_per_second': 283.74, 'eval_steps_per_second': 8.905, 'epoch': 1.14}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in checkpoints/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in checkpoints/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in checkpoints/checkpoint-1000/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, chunk_tags, ner_tags, pos_tags, tokens. If id, chunk_tags, ner_tags, pos_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3250\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.0549, 'learning_rate': 5.333333333333334e-06, 'epoch': 1.25}\n",
            "{'eval_loss': 0.06466855108737946, 'eval_precision': 0.9002918287937743, 'eval_recall': 0.934533826994278, 'eval_f1': 0.9170933113129646, 'eval_accuracy': 0.9805292282333549, 'eval_runtime': 11.1743, 'eval_samples_per_second': 290.846, 'eval_steps_per_second': 9.128, 'epoch': 1.25}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, chunk_tags, ner_tags, pos_tags, tokens. If id, chunk_tags, ner_tags, pos_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3250\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.05, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.37}\n",
            "{'eval_loss': 0.06245027855038643, 'eval_precision': 0.9092398427260813, 'eval_recall': 0.9340289464826658, 'eval_f1': 0.9214677071226964, 'eval_accuracy': 0.9831047271442868, 'eval_runtime': 11.3542, 'eval_samples_per_second': 286.239, 'eval_steps_per_second': 8.983, 'epoch': 1.37}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, chunk_tags, ner_tags, pos_tags, tokens. If id, chunk_tags, ner_tags, pos_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3250\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.0461, 'learning_rate': 2.666666666666667e-06, 'epoch': 1.48}\n",
            "{'eval_loss': 0.06150170788168907, 'eval_precision': 0.909998363606611, 'eval_recall': 0.935880175025244, 'eval_f1': 0.9227578196299676, 'eval_accuracy': 0.9825013245422971, 'eval_runtime': 11.4013, 'eval_samples_per_second': 285.056, 'eval_steps_per_second': 8.946, 'epoch': 1.48}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, chunk_tags, ner_tags, pos_tags, tokens. If id, chunk_tags, ner_tags, pos_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3250\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.0474, 'learning_rate': 1.3333333333333334e-06, 'epoch': 1.59}\n",
            "{'eval_loss': 0.06264644861221313, 'eval_precision': 0.9073983341499265, 'eval_recall': 0.9350387075058902, 'eval_f1': 0.9210111893907998, 'eval_accuracy': 0.9820892447165479, 'eval_runtime': 11.4159, 'eval_samples_per_second': 284.69, 'eval_steps_per_second': 8.935, 'epoch': 1.59}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, chunk_tags, ner_tags, pos_tags, tokens. If id, chunk_tags, ner_tags, pos_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3250\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.0465, 'learning_rate': 0.0, 'epoch': 1.71}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to checkpoints/checkpoint-1500\n",
            "Configuration saved in checkpoints/checkpoint-1500/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.061864305287599564, 'eval_precision': 0.912743972445465, 'eval_recall': 0.9365533490407271, 'eval_f1': 0.9244953899825566, 'eval_accuracy': 0.9828986872314123, 'eval_runtime': 11.3577, 'eval_samples_per_second': 286.15, 'eval_steps_per_second': 8.981, 'epoch': 1.71}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in checkpoints/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in checkpoints/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in checkpoints/checkpoint-1500/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from checkpoints/checkpoint-1500 (score: 0.061864305287599564).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train_runtime': 436.0733, 'train_samples_per_second': 55.037, 'train_steps_per_second': 3.44, 'train_loss': 0.11344004122416178, 'epoch': 1.71}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1500, training_loss=0.11344004122416178, metrics={'train_runtime': 436.0733, 'train_samples_per_second': 55.037, 'train_steps_per_second': 3.44, 'train_loss': 0.11344004122416178, 'epoch': 1.71})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Evaluate trained model\n",
        "\n",
        "We can use the `trainer` to evaluate the trained model using the metric we defined:"
      ],
      "metadata": {
        "id": "SY4dt3HGA-6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate(dataset['test'])\n",
        "\n",
        "pprint(eval_results)\n",
        "\n",
        "print('\\nF1:', eval_results['eval_f1'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3VOmny26WTI",
        "outputId": "93a523a9-446f-490d-c969-7749a200ce02"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, chunk_tags, ner_tags, pos_tags, tokens. If id, chunk_tags, ner_tags, pos_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3453\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.15628966689109802, 'eval_precision': 0.8750213128729752, 'eval_recall': 0.9086402266288952, 'eval_f1': 0.8915139407626163, 'eval_accuracy': 0.9700918674461481, 'eval_runtime': 10.7792, 'eval_samples_per_second': 320.339, 'eval_steps_per_second': 10.019, 'epoch': 1.71}\n",
            "{'epoch': 1.71,\n",
            " 'eval_accuracy': 0.9700918674461481,\n",
            " 'eval_f1': 0.8915139407626163,\n",
            " 'eval_loss': 0.15628966689109802,\n",
            " 'eval_precision': 0.8750213128729752,\n",
            " 'eval_recall': 0.9086402266288952,\n",
            " 'eval_runtime': 10.7792,\n",
            " 'eval_samples_per_second': 320.339,\n",
            " 'eval_steps_per_second': 10.019}\n",
            "\n",
            "F1: 0.8915139407626163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we captured performance during training using the `training_logs` callback, we can also have a look at training and evaluation loss and evaluation $F_1$ progression. (The code here is only for visualization and you do not need to understand it, but you should aim to be able to interpret the plots.)"
      ],
      "metadata": {
        "id": "1jxFdf0uBXYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot(logs, keys, labels):\n",
        "    values = sum([logs[k] for k in keys], [])\n",
        "    plt.ylim(max(min(values)-0.1, 0.0), min(max(values)+0.1, 1.0))\n",
        "    for key, label in zip(keys, labels):    \n",
        "        plt.plot(logs['epoch'], logs[key], label=label)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot(training_logs.logs, ['loss', 'eval_loss'], ['Training loss', 'Evaluation loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "DWLWZBjABEc4",
        "outputId": "20668c92-ce52-40b9-cc91-d2bdeb3f5a0e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot(training_logs.logs, ['eval_f1'], ['Evaluation F1'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "8M1IYKS7Bp6k",
        "outputId": "798a4045-5f5d-4631-fc37-c636fbc2f721"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Create pipeline\n",
        "\n"
      ],
      "metadata": {
        "id": "PWTkJ97rDq7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can wrap our fine-tuned model in a pipeline for convenience. (We need to specify `device` here as the model is on GPU.)"
      ],
      "metadata": {
        "id": "BVlTovCFEgLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = transformers.pipeline(\n",
        "    'token-classification',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    aggregation_strategy='simple',\n",
        "    device=0\n",
        ")"
      ],
      "metadata": {
        "id": "pbw6bQL9Dto0"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then use the pipeline simply as follows:"
      ],
      "metadata": {
        "id": "kxSAAtmAEuAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe('Finnish cities include Turku and Tampere.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTOhvnJ6EIww",
        "outputId": "9f264b64-2331-48aa-e017-97e8efbffb03"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity_group': 'MISC',\n",
              "  'score': 0.9951003,\n",
              "  'word': 'Finnish',\n",
              "  'start': 0,\n",
              "  'end': 7},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': 0.9953265,\n",
              "  'word': 'Turku',\n",
              "  'start': 23,\n",
              "  'end': 28},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': 0.9923725,\n",
              "  'word': 'Tampere',\n",
              "  'start': 33,\n",
              "  'end': 40}]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or, for convenience"
      ],
      "metadata": {
        "id": "Tl1a9_JHIbGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tag(text):\n",
        "    output = pipe(text)\n",
        "    print('input:', text)\n",
        "    print('output:', [(o['word'], o['entity_group']) for o in output])\n",
        "\n",
        "tag('Finnish cities include Turku and Tampere.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsjFMSRwIPye",
        "outputId": "a3a8ece5-3c8b-43d6-8d33-b532d445c010"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: Finnish cities include Turku and Tampere.\n",
            "output: [('Finnish', 'MISC'), ('Turku', 'LOC'), ('Tampere', 'LOC')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tag('Paavo Nurmi was born in Turku in 1897.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI0QzsQ7I7_w",
        "outputId": "3e6867e0-dd82-4ffb-f5a1-04f1671757d3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: Paavo Nurmi was born in Turku in 1897.\n",
            "output: [('Paavo Nurmi', 'PER'), ('Turku', 'LOC')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tag('Nokia is a company founded near the town of Nokia.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqJP87L7JcbT",
        "outputId": "e233a09c-372e-4e79-db18-ce7d720890e6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: Nokia is a company founded near the town of Nokia.\n",
            "output: [('Nokia', 'ORG'), ('Nokia', 'LOC')]\n"
          ]
        }
      ]
    }
  ]
}