{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZPs/y/GTC55Aihbnq0L7v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/textual-data-analysis-course/blob/main/TDA_2023_demo_tasks_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks for the 3rd set of exercises"
      ],
      "metadata": {
        "id": "kgn1ofUdtxb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1\n",
        "\n",
        "On the lecture, we studied an example of zero- and few-shot text classification using a generative language model ([notebook](https://github.com/TurkuNLP/textual-data-analysis-course/blob/main/text_classification_zero_and_few_shot.ipynb)). Building on this example, test whether you can get the model to identify emotions in at least some simple sentences of your own invention. Which prompt formulations work best?\n",
        "\n",
        "You can use any set of emotion labels you like. One possible set is [Plutchik's core emotions](https://en.wikipedia.org/wiki/Robert_Plutchik): Joy, Trust, Fear, Surprise, Sadness, Disgust, Anger, and Anticipation.\n",
        "\n",
        "In addition to running the `opt-1.3b` generative models on Colab that is default in the notebook, try at least one other model. You can do this either in the notebook or using an online interface such as ChatGPT at https://chat.openai.com/ (signup required). Can you detect differences between the models?"
      ],
      "metadata": {
        "id": "0gnk2igzt37H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2\n",
        "\n",
        "We studied examples of text classification resources ([notebook](https://github.com/TurkuNLP/textual-data-analysis-course/blob/main/text_classification_dataset_examples.ipynb)) and supervised monolingual text classification using a Transformer model ([notebook](https://github.com/TurkuNLP/textual-data-analysis-course/blob/main/text_classification_basic_example.ipynb)). Building on these resources and what you previously learned about multi- and cross-lingual models, train a sentiment classifier (`positive`/`negative`) on English data that is capable of classifying texts in other languages. You can either use the `imdb` dataset or any other `positive`/`negative` sentiment dataset.\n",
        "\n",
        "Test your model with at least five sentences of your own invention in a language of your choosing (other than English!). How does the model perform?\n",
        "\n",
        "---\n",
        "\n",
        "**Hint 1**: if you choose `imdb` or another larger dataset, you can speed up processing by randomly downsampling the data with e.g.\n",
        "\n",
        "```\n",
        "dataset = dataset.shuffle()\n",
        "dataset['train'] = dataset['train'].select(range(NEW_SIZE))\n",
        "dataset['test'] = dataset['test'].select(range(NEW_SIZE))\n",
        "\n",
        "```\n",
        "\n",
        "Where `NEW_SIZE` is the size you want to downsample to. You can also cut down on training time by training for fewer steps (`max_steps` in `TrainingArguments`).\n",
        "\n",
        "---\n",
        "\n",
        "**Hint 2**: if your training crashes with the opaque error message `The expanded size of the tensor ... must match the existing size`, you probably need to truncate your data to model maximum length by adding the parameter `truncate=True` to your tokenizer call."
      ],
      "metadata": {
        "id": "hH4X-eenwUxe"
      }
    }
  ]
}