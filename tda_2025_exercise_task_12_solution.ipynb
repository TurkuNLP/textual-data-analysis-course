{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOT2eWCBmwf60LOhlcF5j3j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/textual-data-analysis-course/blob/main/tda_2025_exercise_task_12_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example solution for exercise task 12\n",
        "\n",
        "## Task 12 description\n",
        "\n",
        "To better understand \"reasoning\" LLMs, in this exercise we ask you to read the paper [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2501.12948) and answer the following questions:\n",
        "\n",
        "1. The work involves data that separates \"thinking\" tokens (<code>&lt;think&gt;...&lt;/think&gt;</code>) from answer tokens (<code>&lt;answer&gt;...&lt;/answer&gt;</code>). Where does this data come from?\n",
        "2. The process of creating the models involves the following techniques. Briefly define each and its primary role in the process:\n",
        "  * Reward Modeling and Accuracy vs. format rewards\n",
        "  * Rejection sampling\n",
        "  * Supervised fine-tuning\n",
        "  * Distillation\n",
        "3. The authors use and introduce a number of different models. Briefly define the relationships between the following pairs of models and how information flows between them, relating this to the methods above when relevant:\n",
        "  * DeepSeek-V3-Base and DeepSeek-R1-Zero\n",
        "  * DeepSeek-V3-Base and DeepSeek-R1\n",
        "  * DeepSeek-R1 and DeepSeek-R1-Distill-Llama-70B"
      ],
      "metadata": {
        "id": "dpfXjKv6n62_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Task 12 example solution\n",
        "\n",
        "**Question**: The work involves data that separates \"thinking\" tokens (<code>&lt;think&gt;...&lt;/think&gt;</code>) from answer tokens (<code>&lt;answer&gt;...&lt;/answer&gt;</code>). Where does this data come from?\n",
        "\n",
        "**Answer**: this data is initially generated by the model itself, starting from DeepSeek-V3-Base and using the prompt template\n",
        "\n",
        "    A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n",
        "    The assistant first thinks about the reasoning process in the mind and then provides the user\n",
        "    with the answer. The reasoning process and answer are enclosed within <think> </think> and\n",
        "    <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\n",
        "    <answer> answer here </answer>. User: prompt. Assistant:\n",
        "\n",
        "in a reinforcement learning setup to create DeepSeek-R1-Zero. For DeepSeek-R1, the authors use a combination of data sources (section 2.3.1):\n",
        "\n",
        "    To collect such data, we have explored several approaches: using few-shot\n",
        "    prompting with a long CoT as an example, directly prompting models to\n",
        "    generate detailed answers with reflection and verification, gathering\n",
        "    DeepSeek-R1- Zero outputs in a readable format, and refining the results\n",
        "    through post-processing by human annotators."
      ],
      "metadata": {
        "id": "kUm3zTdxpGKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: The process of creating the models involves the following techniques. Briefly define each and its primary role in the process:\n",
        "  * Reward Modeling and Accuracy vs. format rewards\n",
        "  * Rejection sampling\n",
        "  * Supervised fine-tuning\n",
        "  * Distillation\n",
        "\n",
        "**Answer**:\n",
        "  * **Reward Modeling**: \"reward\" is the numerical value that serves as the training signal for reinforcement learning. The authors apply two types of rewards: **accuracy reward** is given if the answer is correct, and **format reward** if the answer has the correct format (<code>&lt;think&gt;...&lt;/think&gt;</code> and <code>&lt;answer&gt;...&lt;/answer&gt;</code>). These rewards guide the model to develop \"reasoning\" capability.\n",
        "  * **Rejection sampling**: in brief, generate multiple outputs and filter out undesirable ones. This is used to collect high-quality reasoning examples for supervised fine-tuning.\n",
        "  * **Supervised fine-tuning**: in this context, continued training of a model on high-quality data. **NOTE**: despite the name, the data doesn't consist of input-output pairs (x, y). Used to introduce specific capabilities to the model.\n",
        "  * **Distillation**: training a smaller \"student\" model using a larger \"teacher\" model. Used to introduce \"reasoning\" and other capabilities from R1 to models such as Llama through \"black-box\" distillation where the student is trained on teacher ouputs without direct access to the teacher model."
      ],
      "metadata": {
        "id": "MBXk1NI-qYjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: The authors use and introduce a number of different models. Briefly define the relationships between the following pairs of models and how information flows between them, relating this to the methods above when relevant:\n",
        "* DeepSeek-V3-Base and DeepSeek-R1-Zero\n",
        "* DeepSeek-V3-Base and DeepSeek-R1\n",
        "* DeepSeek-R1 and DeepSeek-R1-Distill-Llama-70B\n",
        "\n",
        "**Answer**:\n",
        "* **DeepSeek-V3-Base and DeepSeek-R1-Zero**: DeepSeek-R1-Zero is derived from DeepSeek-V3-Base using pure reinforcement learning (RL) with the presented **reward model**.\n",
        "* **DeepSeek-V3-Base and DeepSeek-R1**: DeepSeek-R1 is derived from DeepSeek-V3-Base using a combination of **supervised fine-tuning** using curated \"cold-start\" data and data generated through **rejection sampling** and RL with the presented **reward model**.\n",
        "* **DeepSeek-R1 and DeepSeek-R1-Distill-Llama-70B**: DeepSeek-R1-Distill-Llama-70B is a smaller model **distilled** from DeepSeek-R1."
      ],
      "metadata": {
        "id": "7kujOkSls5UH"
      }
    }
  ]
}