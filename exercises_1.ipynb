{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/textual-data-analysis-course/blob/main/exercises_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YLE news data\n",
        "\n",
        "* We'll be using data from YLE news for many of the demos\n",
        "* This data is gathered from 2021 YLE RSS feed\n",
        "* It's here: http://dl.turkunlp.org/TKO_8964_2023/"
      ],
      "metadata": {
        "id": "1vuRpBcmorYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1\n",
        "\n",
        "* Grab the data with `wget` and give it a look\n",
        "* Think what kinds of NLP tasks you could use the data for?"
      ],
      "metadata": {
        "id": "3O-yE24Ip-Yb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2\n",
        "\n",
        "* Load the data as a HuggingFace dataset\n",
        "* Remember to pip-install datasets first\n",
        "* https://huggingface.co/docs/datasets/loading#json"
      ],
      "metadata": {
        "id": "cX-5BJlXqWAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install datasets"
      ],
      "metadata": {
        "id": "aZUQTDCfquLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3\n",
        "\n",
        "- Try some keyword search on the data\n",
        "- Run the data through the trusty CountVectorizer\n",
        "- Do some keyword search, e.g. look for \"Turku\" AND \"silta\" and print the titles of the news that match (do the search on the text field, though)\n",
        "- Some hints:\n",
        "  - if you just copy the code from the lecture, you will get out-of-memory error\n",
        "  - can you think why? what is the data structure returned by the vectorizer?\n",
        "  - the vocabulary of the vectorizer is in `cv.vocabulary_`\n",
        "  - you may still need `.todense()` but maybe in a different spot\n",
        "  - you may need `.nonzero()` to gather the matching documents\n",
        "  \n"
      ],
      "metadata": {
        "id": "oyRhxtGzspNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4\n",
        "\n",
        "* This is for those who run a little ahead\n",
        "* Run the dataset through a HuggingFace pipeline as follows:\n",
        "  * Model: `xlm-roberta-base` (or any other similar model of your choice)\n",
        "  * Task: `feature-extraction`\n",
        "  * Relevant documentation: https://huggingface.co/docs/transformers/v4.24.0/en/main_classes/pipelines#pipeline-batching\n",
        "  * You will likely want to use the GPU\n",
        "  * Make sure you understand what the return values are\n"
      ],
      "metadata": {
        "id": "OjzFj0LpzZ8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install transformers"
      ],
      "metadata": {
        "id": "FcVGOtEE0Jj4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}