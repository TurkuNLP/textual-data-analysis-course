{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOv5+tygSQYH+C3Gkc+QIl/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/textual-data-analysis-course/blob/main/text_classification_zero_and_few_shot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero- and few-shot text classification\n",
        "\n",
        "Experiment on zero- and few-shot text classification with a large generative language model."
      ],
      "metadata": {
        "id": "XUtuGZ3CbwCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Setup\n",
        "\n",
        "Install the required python package"
      ],
      "metadata": {
        "id": "_LMdMT4mcF3B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LWcYPF6IXX0D"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the `transformers` library"
      ],
      "metadata": {
        "id": "a-BxELZWcOcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "TxKn8a_mXaXq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Load model\n",
        "\n",
        "We'll load a variant of the [Open Pretrained Transformer](https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/) (OPT).\n",
        "\n",
        "Models ranging from 125M parameters (roughly BERT-base size) to 66B parameters are currently available from the [Hugging Face repository](https://huggingface.co/models?sort=downloads&search=facebook%2Fopt).\n"
      ],
      "metadata": {
        "id": "z9HRIu8Bca9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'facebook/opt-1.3b'\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(MODEL)\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL)"
      ],
      "metadata": {
        "id": "w3yDLLpOXeos"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Generation\n",
        "\n",
        "We'll use `model.generate()` in a simple function that tokenizes and vectorizes a text prompt, generates up to a given maximum of new tokens, decodes the output back into text, and returns a string with the prompt marked with `**`: "
      ],
      "metadata": {
        "id": "7wPkLzyyeh3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt, max_new_tokens=10):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    outputs = model.generate(inputs.input_ids, max_new_tokens=max_new_tokens)\n",
        "    decoded = tokenizer.batch_decode(\n",
        "        outputs,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False\n",
        "    )\n",
        "    output_text = decoded[0]\n",
        "    generated = output_text[len(prompt):]\n",
        "    return(f'**{prompt}**{generated}')"
      ],
      "metadata": {
        "id": "10xgSAi-YOQU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a general generative model, so we can prompt it with any text we like."
      ],
      "metadata": {
        "id": "O_CobHB2eSkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate('Hi OPT, how are you doing today?'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FpaI-TPeYg6",
        "outputId": "d214dbc8-c46a-40de-afa7-e1b8f8df4f12"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Hi OPT, how are you doing today?**\n",
            "I'm doing great! How are you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Zero-shot experiment\n",
        "\n",
        "In a zero-shot setting, the prompt is a natural language formulation of the task:"
      ],
      "metadata": {
        "id": "PpSnABzleuXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate('''\n",
        "Is this review positive or negative?\n",
        "Review: \"This movie sucks!\"\n",
        "Answer:'''\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqqYZ76UZvB8",
        "outputId": "a2a9d2a4-091c-4553-e8c2-92f0aed9f4c4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**\n",
            "Is this review positive or negative?\n",
            "Review: \"This movie sucks!\"\n",
            "Answer:** \"This movie sucks!\"\n",
            "\n",
            "Is this review\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perhaps unsurprisingly, the model does not answer as hoped. Unlike e.g. ChatGPT, this model is not pre-trained with dialogue data, but rather as a standard language model. (The only way the model could have learned question-answer pairs is if they happened to coincidentally appear in its training data.)"
      ],
      "metadata": {
        "id": "FhAB6fLvfNdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## One-shot experiment\n",
        "\n",
        "In a one-shot setting, the prompt includes a single example of a correct question-answer pair before the question we're actually interested in.\n",
        "\n",
        "In a sense, here `{ 'text': 'This movie sucks!', 'label': 'Negative' }` is our one and only \"training\" example."
      ],
      "metadata": {
        "id": "VrYKJZgYgSeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate('''\n",
        "Is this review positive or negative?\n",
        "Review: \"This movie sucks!\"\n",
        "Answer: Negative\n",
        "\n",
        "Is this review positive or negative?\n",
        "Review: \"This movie is great!\"\n",
        "Answer:'''\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Fi6FvpoZ8sj",
        "outputId": "fd8fb368-2da7-4d42-8d87-f98e3d8f0c9f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**\n",
            "Is this review positive or negative?\n",
            "Review: \"This movie sucks!\"\n",
            "Answer: Negative\n",
            "\n",
            "Is this review positive or negative?\n",
            "Review: \"This movie is great!\"\n",
            "Answer:** Positive\n",
            "\n",
            "Is this review positive or negative?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That actually _kind_ of worked! The first word after our prompt is `\"Negative\"`, which is the correct answer.\n",
        "\n",
        "Of course, the model doesn't know to stop there and continues instead to produce what it predicts is the most likely continuation, namely the start of a third question. (Not an unreasonable assumption.)"
      ],
      "metadata": {
        "id": "XR1k95Tyhxp3"
      }
    }
  ]
}