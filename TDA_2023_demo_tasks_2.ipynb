{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbstK5GLY6l6kMjEljO1MB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/textual-data-analysis-course/blob/main/TDA_2023_demo_tasks_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks for the 2nd set of exercises"
      ],
      "metadata": {
        "id": "WptYYSjyte7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1\n",
        "\n",
        "At the end of the lecture, we were able to see how retrieval can be implemented with a Transformer. Read through the notebook linked on moodle and make sure you genuinely understand what's going on in it. Ask if you do not. Then,\n",
        "\n",
        "1. Test this model `sentence-transformers/paraphrase-xlm-r-multilingual-v1` on the same task and same data.\n",
        "2. Compare its results to the model named `xlm-roberta-base` which is a classic multi- (but not cross-) lingual model. Does it do any better? And if yes, in what way? Discuss this with a friend on your left or right. :) \n",
        "3. You can do this also on Finnish, since both of these models are multilingual and know Finnish\n",
        "\n",
        "Basically, all you need to do is to grab the notebook from the course and change the model name. If you switch to Finnish, be prepared the run takes a while, as you will be pushing some 20K news through a large Transformer. Make sure the GPU is enabled!\n",
        "\n",
        "# Task 2\n",
        "\n",
        "The model `sentence-transformers/paraphrase-xlm-r-multilingual-v1` is cross-lingual, i.e. it should provide embeddings that are comparable across languages. This means that we will be able to align the Finnish and English news in the data based on their headlines (titles). I will explain how this model is trained next Monday, but you can now simply test it and see if it works as advertised.\n",
        "\n",
        "For this task, you need to minimally modify the notebook from Monday lecture so that English is not compared to itself, but to Finnish. And then you adjust the little details to make everything work. In terms of code, these are tiny changes.\n",
        "\n",
        "When you look at the cross-lingual pairs produced by the sentence-transformer as opposed to xlm-roberta-base, do you observe any substantial difference in results? (hint: I did)\n"
      ],
      "metadata": {
        "id": "0hftvqJati7n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbu9wA5iteaF"
      },
      "outputs": [],
      "source": []
    }
  ]
}